{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gensim.models import word2vec\n",
    "import networkx as nx \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import xml.dom.minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "from GCN import *\n",
    "import community\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import *\n",
    "import csv\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "import copy\n",
    "\n",
    "\n",
    "class AliasSampling:\n",
    "    def __init__(self, prob):\n",
    "        self.n = len(prob)\n",
    "        self.U = np.array(prob) * self.n\n",
    "        self.K = [i for i in range(len(prob))]\n",
    "        overfull, underfull = [], []\n",
    "        for i, U_i in enumerate(self.U):\n",
    "            if U_i > 1:\n",
    "                overfull.append(i)\n",
    "            elif U_i < 1:\n",
    "                underfull.append(i)\n",
    "        while len(overfull) and len(underfull):\n",
    "            i, j = overfull.pop(), underfull.pop()\n",
    "            self.K[j] = i\n",
    "            self.U[i] = self.U[i] - (1 - self.U[j])\n",
    "            if self.U[i] > 1:\n",
    "                overfull.append(i)\n",
    "            elif self.U[i] < 1:\n",
    "                underfull.append(i)\n",
    "\n",
    "    def sampling(self, n=1):\n",
    "        x = np.random.rand(n)\n",
    "        i = np.floor(self.n * x)\n",
    "        y = self.n * x - i\n",
    "        i = i.astype(np.int32)\n",
    "        res = [i[k] if y[k] < self.U[i[k]] else self.K[i[k]] for k in range(n)]\n",
    "        if n == 1:\n",
    "            return res[0]\n",
    "        else:\n",
    "            return res\n",
    "        \n",
    "        \n",
    "def GHAC(mlist,G,idx_pid,n_clusters=-1):\n",
    "        \n",
    "    distance=[]\n",
    "    graph=[]\n",
    "    for i in range(len(mlist)):\n",
    "        gtmp=[]\n",
    "        for j in range(len(mlist)):\n",
    "            if i<j and G.has_edge(idx_pid[i],idx_pid[j]):\n",
    "                cosdis=1/(1+np.exp(-np.dot(mlist[i],mlist[j])))\n",
    "                gtmp.append(cosdis)\n",
    "            elif i>j:\n",
    "                gtmp.append(graph[j][i])\n",
    "            else:\n",
    "                gtmp.append(0)\n",
    "        graph.append(gtmp)\n",
    "        \n",
    "    graph=np.array(graph)\n",
    "    distance =np.multiply(graph,-1)\n",
    "    \n",
    "    if n_clusters==-1:\n",
    "        best_m=-10000000\n",
    "        \n",
    "        n_components, labels = connected_components(graph) \n",
    "        Gr=nx.from_numpy_matrix(graph)\n",
    "        \n",
    "        graph[graph<=0.9]=0 #Edge pre-clustering \n",
    "        n_components1, labels = connected_components(graph)\n",
    "        \n",
    "        for k in range(n_components1,n_components-1,-1):  \n",
    "            model_HAC = AgglomerativeClustering(linkage=\"average\",affinity='precomputed',n_clusters=k)\n",
    "            model_HAC.fit(distance)\n",
    "            labels = model_HAC.labels_\n",
    "            \n",
    "            part= {}\n",
    "            for j in range (len(labels)):\n",
    "                part[j]=labels[j]\n",
    "\n",
    "            mod = community.modularity(part,Gr)\n",
    "            if mod>=best_m:\n",
    "                best_m=mod\n",
    "                best_labels=labels\n",
    "        labels = best_labels\n",
    "    else:\n",
    "        model_HAC = AgglomerativeClustering(linkage=\"average\",affinity='precomputed',n_clusters=n_clusters)\n",
    "        model_HAC.fit(distance)\n",
    "        labels = model_HAC.labels_\n",
    "    \n",
    "    return labels\n",
    "  \n",
    "\n",
    "def pairwise_evaluate(correct_labels,pred_labels):\n",
    "    TP = 0.0  # Pairs Correctly Predicted To SameAuthor\n",
    "    TP_FP = 0.0  # Total Pairs Predicted To SameAuthor\n",
    "    TP_FN = 0.0  # Total Pairs To SameAuthor\n",
    "\n",
    "    for i in range(len(correct_labels)):\n",
    "        for j in range(i + 1, len(correct_labels)):\n",
    "            if correct_labels[i] == correct_labels[j]:\n",
    "                TP_FN += 1\n",
    "            if pred_labels[i] == pred_labels[j]:\n",
    "                TP_FP += 1\n",
    "            if (correct_labels[i] == correct_labels[j]) and (pred_labels[i] == pred_labels[j]):\n",
    "                TP += 1\n",
    "\n",
    "    if TP == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = TP / TP_FP\n",
    "        pairwise_recall = TP / TP_FN\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "    return pairwise_precision, pairwise_recall, pairwise_f1\n",
    "\n",
    "\n",
    "save_model_name = \"gene/word2vec.model\"\n",
    "model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～]+'\n",
    "stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','method','algrithom','by','model']\n",
    "stopword = [porter_stemmer.stem(w) for w in stopword]\n",
    "\n",
    "result=[]\n",
    "\n",
    "path = \"raw-data/\"\n",
    "file_names = os.listdir(path)\n",
    "for fname in file_names:\n",
    "    # Data processing\n",
    "    fname = fname[:-4]\n",
    "    f = open(path + fname + \".xml\",'r',encoding = 'utf-8').read()\n",
    "    text=re.sub(u\"&\",u\" \",f)\n",
    "    root = ET.fromstring(text)\n",
    "    \n",
    "    correct_labels=[]\n",
    "    p_to={} #Original title\n",
    "    p_t={} #processed to word stem\n",
    "\n",
    "    for i in root.findall('publication'):\n",
    "        pid = i.find('id').text\n",
    "        \n",
    "        if pid in p_t:\n",
    "            pid = pid+'1'\n",
    "        \n",
    "        correct_labels.append(int(i.find('label').text))\n",
    "               \n",
    "        line = i.find('title').text\n",
    "        line = re.sub(r, ' ', line)\n",
    "        line = line.replace('\\t',' ')\n",
    "        line = line.lower()\n",
    "        split_cut = line.split(' ')\n",
    "        \n",
    "        p_t[pid]=[]\n",
    "        p_to[pid]=[]\n",
    "        for j in split_cut:\n",
    "            if len(j)>1:\n",
    "                p_to[pid].append(j)\n",
    "                if porter_stemmer.stem(j) not in stopword:\n",
    "                    p_t[pid].append(porter_stemmer.stem(j))\n",
    "\n",
    "                    \n",
    "                    \n",
    "    # Construct PHNet            \n",
    "    pid_idx={}\n",
    "    idx_pid={}\n",
    "    idx=0\n",
    "    G = nx.Graph()\n",
    "    for pid in p_t:\n",
    "        G.add_node(pid)\n",
    "        pid_idx[pid]=idx\n",
    "        idx_pid[idx]=pid\n",
    "        idx=idx+1\n",
    "        \n",
    "    ## CoAuthor\n",
    "    Ga = nx.Graph()\n",
    "    for pid in p_t:\n",
    "        Ga.add_node(pid)\n",
    "    fa = open(\"experimental-results/authors/\" + fname + \"_authorlist.txt\",'r',encoding = 'utf-8').readlines()  \n",
    "    for line in fa:\n",
    "        line.strip()\n",
    "        split_cut = line.split('\\t')\n",
    "        keyi = idx_pid[int(split_cut[0].strip())]\n",
    "        keyj = idx_pid[int(split_cut[1].strip())]\n",
    "        weights = 1\n",
    "        if Ga.has_edge(keyi,keyj):\n",
    "            Ga[keyi][keyj]['weight'] =Ga[keyi][keyj]['weight'] + weights\n",
    "        else:\n",
    "            Ga.add_edge(keyi,keyj,{'weight': weights})        \n",
    "\n",
    "    ## CoVenue\n",
    "    Gv = nx.Graph()\n",
    "    for pid in p_t:\n",
    "        Gv.add_node(pid)\n",
    "    fv = open(\"experimental-results/\" + fname + \"_jconfpair.txt\",'r',encoding = 'utf-8').readlines()  \n",
    "    for line in fv:\n",
    "        line.strip()\n",
    "        split_cut = line.split('\\t')\n",
    "        keyi = idx_pid[int(split_cut[0].strip())]\n",
    "        keyj = idx_pid[int(split_cut[1].strip())]\n",
    "        weights = 1\n",
    "        Gv.add_edge(keyi,keyj,{'weight': weights})        \n",
    "\n",
    "    ## CoTitle\n",
    "    Gt = nx.Graph()\n",
    "    for pid in p_t:\n",
    "        Gt.add_node(pid)\n",
    "    for i, keyi in enumerate(p_t):\n",
    "        for j, keyj in enumerate(p_t): \n",
    "            weights=len(set(p_t[keyi]).intersection(set(p_t[keyj])))\n",
    "            if (j>i and weights>=2):\n",
    "                Gt.add_edge(keyi,keyj,{'weight': weights}) \n",
    "                    \n",
    "\n",
    "    Glist=[]\n",
    "    Glist.append(Ga)   \n",
    "    Glist.append(Gt)\n",
    "    Glist.append(Gv)\n",
    "                   \n",
    "    for i in range(len(Glist)):\n",
    "        for u,v,d in Glist[i].edges(data = 'weight'): \n",
    "            if G.has_edge(u,v):\n",
    "                G[u][v]['weight'] = G[u][v]['weight'] + d['weight']\n",
    "            else:\n",
    "                G.add_edge(u,v,{'weight': d['weight']}) \n",
    "    Glist.append(G)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Sampling\n",
    "    all_neighbor_samplings=[]\n",
    "    all_neg_sampling=[]\n",
    "\n",
    "    for i,Gi in enumerate(Glist):\n",
    "        adj_matrix = nx.adj_matrix(Gi).toarray()\n",
    "        \n",
    "        Gtmp= copy.deepcopy(Gi)\n",
    "        for u,v,d in Gtmp.edges(data = 'weight'):\n",
    "            Gtmp[u][v]['weight'] = 1\n",
    "        length = nx.all_pairs_dijkstra_path_length(Gtmp)\n",
    "        \n",
    "        for u in length:\n",
    "            for v in length[u]:\n",
    "                if Gtmp.has_edge(u,v) is False and length[u][v]>0:\n",
    "                    Gtmp.add_edge(u,v,{'weight': length[u][v]})\n",
    "        pathl_matrix = nx.adj_matrix(Gtmp).toarray()\n",
    "\n",
    "        neighbor_samplings = []\n",
    "        neg_samplings=[]\n",
    "        for i in range(G.number_of_nodes()):\n",
    "            node_weights = adj_matrix[i]\n",
    "            if np.sum(node_weights)==0:\n",
    "                neighbor_samplings.append(0)\n",
    "            else :\n",
    "                weight_distribution = node_weights / np.sum(node_weights)            \n",
    "                neighbor_samplings.append(AliasSampling(weight_distribution))\n",
    "                \n",
    "            node_i_degrees = pathl_matrix[i]\n",
    "            node_i_degrees[node_i_degrees==0] = 6\n",
    "            node_i_degrees[i]=0\n",
    "            node_i_degrees[node_i_degrees<=1] = 0\n",
    "\n",
    "            if np.sum(node_i_degrees)==0:\n",
    "                neg_samplings.append(0)\n",
    "            else:\n",
    "                node_distribution = node_i_degrees / np.sum(node_i_degrees)\n",
    "                neg_samplings.append(AliasSampling(node_distribution))\n",
    "                      \n",
    "        all_neighbor_samplings.append(neighbor_samplings)\n",
    "        all_neg_sampling.append(neg_samplings)\n",
    "\n",
    "    numwalks=4\n",
    "    walklength=10\n",
    "    negative_num=3\n",
    "\n",
    "    u_i=[]\n",
    "    u_j=[]\n",
    "    label=[]\n",
    "    metapath=[0,1,0,2]\n",
    "    \n",
    "    for node_index in range(G.number_of_nodes()):       \n",
    "        for j in range(0, numwalks):\n",
    "            node_start=node_index\n",
    "            g_index=j\n",
    "            gi=metapath[g_index]\n",
    "            for i in range(0, walklength):     \n",
    "                if all_neighbor_samplings[gi][node_start] != 0: \n",
    "                    node_p = all_neighbor_samplings[gi][node_start].sampling()\n",
    "                    u_i.append(node_start)\n",
    "                    u_j.append(node_p)\n",
    "                    label.append(1)\n",
    "\n",
    "                    if all_neg_sampling[-1][node_start] != 0:\n",
    "                        for k in range(negative_num):\n",
    "                            node_n = all_neg_sampling[-1][node_start].sampling()\n",
    "                            u_i.append(node_start)\n",
    "                            u_j.append(node_n)\n",
    "                            label.append(-1)\n",
    "                        \n",
    "                    g_index=(g_index+1)%len(metapath)\n",
    "                    gi=metapath[g_index]\n",
    "                    \n",
    "                    if all_neighbor_samplings[gi][node_p] != 0:\n",
    "\n",
    "                        node_p1 = all_neighbor_samplings[gi][node_p].sampling()\n",
    "                        u_i.append(node_start)\n",
    "                        u_j.append(node_p1)\n",
    "                        label.append(1)\n",
    "                        if all_neg_sampling[-1][node_start] != 0:\n",
    "                            for k in range(negative_num):\n",
    "                                node_n = all_neg_sampling[-1][node_start].sampling()\n",
    "                                u_i.append(node_start)\n",
    "                                u_j.append(node_n)\n",
    "                                label.append(-1)\n",
    "                  \n",
    "                    node_start = node_p\n",
    "                    \n",
    "                else:\n",
    "                    for k in range(negative_num):\n",
    "                        node_n = all_neg_sampling[-1][node_start].sampling()\n",
    "                        u_i.append(node_start)\n",
    "                        u_j.append(node_n)\n",
    "                        label.append(-1)\n",
    "                    g_index=(g_index+1)%len(metapath)\n",
    "                    gi=metapath[g_index]\n",
    "\n",
    "                    \n",
    "                    \n",
    "    # Training    \n",
    "    node_attr = []\n",
    "    for pid in p_to:\n",
    "        words_vec=[]\n",
    "        for word in p_to[pid]:\n",
    "            if (word in model_w):\n",
    "                words_vec.append(model_w[word])\n",
    "        if len(words_vec)==0:\n",
    "            words_vec.append(2*np.random.random(100)-1)\n",
    "        node_attr.append(np.mean(words_vec,0))\n",
    "    node_attr=np.array(node_attr)\n",
    "\n",
    "    batch_size = 64\n",
    "    total_batch = 3*int(len(u_i)/batch_size)\n",
    "    display_batch = 100\n",
    "\n",
    "    model = GCN(Glist, node_attr, batch_size=batch_size)\n",
    "\n",
    "    avg_loss = 0.\n",
    "    for i in range(total_batch):\n",
    "        sdx=(i*batch_size)%len(u_i)\n",
    "        edx=((i+1)*batch_size)%len(u_i)\n",
    "        #print (sdx,edx)\n",
    "        if edx>sdx:\n",
    "            u_ii = u_i[sdx:edx]\n",
    "            u_jj = u_j[sdx:edx]\n",
    "            labeli = label[sdx:edx]\n",
    "        else:\n",
    "            u_ii = u_i[sdx:]+u_i[0:edx]\n",
    "            u_jj = u_j[sdx:]+u_j[0:edx]\n",
    "            labeli = label[sdx:]+label[0:edx]\n",
    "        loss= model.train_line(u_ii, u_jj, labeli)\n",
    "        avg_loss += loss / display_batch\n",
    "        if i % display_batch == 0 and i > 0:\n",
    "            print ('%d/%d loss %8.6f' %(i,total_batch,avg_loss))\n",
    "            avg_loss = 0.\n",
    "    \n",
    "\n",
    "    \n",
    "    # Evaluating\n",
    "    embed_matrix = model.cal_embed()\n",
    "    labels = GHAC(embed_matrix,Glist[-1],idx_pid,len(set(correct_labels)))\n",
    "    pairwise_precision, pairwise_recall, pairwise_f1 = pairwise_evaluate(correct_labels,labels)\n",
    "    result.append([fname,pairwise_precision, pairwise_recall, pairwise_f1])\n",
    "    print (correct_labels,len(set(correct_labels)))\n",
    "    print (list(labels),len(set(list(labels))))\n",
    "    print (fname,pairwise_precision, pairwise_recall, pairwise_f1)\n",
    "\n",
    "    \n",
    "\n",
    "# Macro-F1\n",
    "Prec = 0\n",
    "Rec = 0\n",
    "F1 = 0    \n",
    "save_csvpath = 'result/'\n",
    "with open(save_csvpath+'AM_nok.csv','w',newline='',encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"name\",\"Prec\",\"Rec\",\"F1\"])\n",
    "    for i in result:\n",
    "        Prec = Prec + i[1]\n",
    "        Rec = Rec + i[2]\n",
    "        F1 = F1 + i[3]\n",
    "    Prec = Prec/len(result)\n",
    "    Rec = Rec/len(result)\n",
    "    F1 = F1/len(result)\n",
    "    writer.writerow([\"Avg\",Prec,Rec,F1])\n",
    "    for i in range(len(result)):\n",
    "        tmp = result[i]\n",
    "        writer.writerow(tmp[0:4])\n",
    "\n",
    "print (\"Avg\",Prec,Rec,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
